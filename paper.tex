\documentclass[conference]{IEEEtran}
\makeatletter
\def\ps@headings{%
\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}%
\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}%
\def\@oddfoot{}%
\def\@evenfoot{}}
\makeatother
\pagestyle{headings}

\newcommand{\thetitle}{On the Linear Structure of Network Traffic: Predicting Flow Behavior}

\input{packages}
\input{definitions}

\title{\vspace{-0.25em}\thetitle}
\author{
{\large{Stefan~Karpinski, John~R.~Gilbert, Elizabeth~M.~Belding}} \vspace{0.25em}\\
Department of Computer Science \\
University of California, Santa Barbara \vspace{0.35em}\\
\textit{\{sgk,gilbert,ebelding\}@cs.ucsb.edu}
}

\bibliographystyle{IEEEtran}

\newcommand{\figurename}{Figure}
\newcommand{\tablename}{Table}

\begin{document}
\maketitle

\section{Introduction}\sec{introduction}

\newfootnote{\flownote}{We use the common definition of a \textit{flow} as a sequence of packets sharing the same  ``5-tuple'': IP protocol type, source and destination nodes, and TCP/UDP port numbers.}

This paper is the first in a three-part series, employing non-parametric discrete mixture modeling to understand and analyze network traffic patterns at the level of individual flows and packets.\flownote
All three papers employ the same theoretical framework but investigate different fundamental applications of traffic modeling:
prediction, classification and generation.
In this paper we first present the framework and then demonstrate how it can predict detailed behavior of individual flows from the observation of only a handful of initial packets.
% The following papers will focus on classification of flows according to application type and realistic workload generation.

The fundamental concept underlying this work is \emph{linear representation of network traffic}~\cite{Karpinski08}.
A linear representation is a function that maps sets of flows to vectors, the coordinates of which describe aspects of flow behavior;
it must distribute additively over set unions.
% the representation must also satisfy a simple linearity condition which we will describe in detail later.
% the sum of the representations of a set of flows is a vector that represents the aggregate behavior of the set of flows.
An entire traffic trace may be represented as a matrix whose rows are the representations of flows in the trace.
We restrict our consideration to a particularly natural and powerful class of representations whose coordinates are the frequencies of flow features in a set of flows.
For example, the representation can have a dimension for each packet size up to 1500; coordinates in these dimensions indicate the number of packets of each size.
Port numbers, \caps{IP} protocol type, and inter-packet intervals can be handled similarly.
% Examples of flow features are the number of packets of a certain size or the use of a particular port number.
% The fundamental premise of this work, and of the sequel, is that the representatives of actual flow behaviors can be understood as being sampled from 

The essential observation of this research is that under such representations, real flows tend to lie near a small set of low-rank subspaces.
This property allows us to perform a drastic model reduction on trace traffic patterns while preserving the essential characteristics of the original traffic.
Moreover, when observable characteristics of flows are separated from features to be predicted, this model reduction is similar to a classical linear regression:
it allows us to use a least squares optimization on observed features to predict behavior.
We use this technique to predict the following characteristics of each flow from observing only a handful of its initial packets:
\begin{enumerate}
  \item the distribution of packets sizes,
  \item the distribution of inter-packet intervals,
  \item the number of packets that will be in the flow.
\end{enumerate}
From these properties we can indirectly predict many other properties, including the total duration and data volume.

At this point, it may be wise to address what we mean here by prediction.
Brief reflection shows that one cannot hope to accurately predict the exact behavior of flows:
flow behavior is nondeterministic---flows may exhibit the same initial behavior but subsequently behave quite differently.
The best we can do then is to predict a \emph{distribution} of possible outcomes given the observed initial behavior.
When compared with a specific outcome, such a prediction cannot be said to be ``wrong'' or ``right.''
How then can one measure the quality of a predicted distribution of outcomes?
This subtle and difficult issue is central to our evaluation methodology.

% real-world network traces tend to have approximately low-rank representation matrices despite the potential rank of such matrices being very large.
% Moreover, most flows' vectors lie on only one of a few even lower rank subspaces.
% Moreover, the low-rank subspace near which real traffic lies can be decomposed into subspaces of even lower rank, with almost all flows lying in one of these subspaces.
% Typically, these smaller subspaces are one-dimensional, but in some cases they may have up to a dozen dimensions.

% the vast majority of observed flow behaviors in any given trace lie in one of a handful low-rank subspaces.
% These subspaces are mostly of rank one, but a few have dimensions as high as ten.

% Karpinski~\emph{et~al.}~\cite{Karpinski08} further observed that the simplifications commonly used in traffic modeling correspond to multiplication of traffic representation matrices by low-rank matrices, averaging over rows or columns.
% They demonstrated experimentally that such simplifications severely distort important behavioral properties of traffic traces;
% viewing these simplification as low-rank matrix transformations, this is unsurprising:
% most of the detail of the original traffic behavior is destroyed.
% It was suggested that matrix factorization provides an alternative and superior means of model reduction.

\section{Background \& Motivation}\sec{background}\sec{motivation}

\newfootnote{\svdnote}{More precisely, the product of the first $k$ columns of $U$ with the first $k$ rows of $\trans{V}$ is a Frobenius-optimal rank-$k$ approximation of $X$.}

\newfootnote{\projectionnote}{We could normalize the vectors to unit length, but then our projection would be spherical, artificially introducing curvature into our visualization.}

The foremost tool for investigating linear structure in high dimensions is the singular value decomposition (\caps{SVD}).
This matrix decomposition can be applied to any real matrix and factors it into a product of three real matrices:
\begin{align}
  X = US\trans{V}.
\end{align}
Here $U$ and $V$ are orthogonal and $S$ is diagonal, with entries of decreasing nonnegative value.
Among the useful properties of this factorization, is that the first $k$ columns of $US$ are an optimal $k$-dimensional reduced model of the rows of $X$, in a new coordinate system given by the first $k$ columns of $V$\!.\,\svdnote

This property allows us to visualize the approximate structure of $X$ using initial columns of $US$.
We are, however, for the moment only interested in direction not magnitude:
vectors with the same direction lie in the same one-dimensional subspace.
To visualize direction while eliminating distance, we project all data points onto the unit-sum hyperplane through the origin.\projectionnote
\Figure{svd} shows scatter plots of the most significant two projected \caps{SVD} coordinates for four different traces of network traffic.
The original data encodes packet size distributions, inter-packet interval distributions, \caps{IP} protocol numbers, and port numbers for a sample of 5000 flows from each trace.
The exact details of how the linear representation encodes these features will be discussed in \Section{methodology}.

\begin{figure}[!t]
\vspace{0.5em}
\begin{center}
\subfloat[\footnotesize{IETF 60}]{\includegraphics[width=1.6in]{svd/ie60}}
\subfloat[\footnotesize{IETF 67}]{\includegraphics[width=1.6in]{svd/ie67}}
\subfloat[\footnotesize{SIGCOMM 2004}]{\includegraphics[width=1.6in]{svd/sc04}}
\subfloat[\footnotesize{UCSD}]{\includegraphics[width=1.6in]{svd/ucsd}}
\caption{SVD scatter plots for various traffic traces.} 
\fig{svd}
\end{center}
\vspace{-2em}
\end{figure}

Linear structure is clearly visible in \Figure{svd}:
the points are heavily concentrated on low-dimensional subspaces.
Because of the projection step of our process, points concentrated along a one-dimensional subspace appear as dark ``spots'' where many points have been plotted;
points concentrated along a two-dimensional subspace appear as lines;
points falling near subspaces of more dimensions appear as ``smears.''

\section{Methodology}\sec{methodology}

\section{Results}\sec{results}

\section{Discussion}\sec{discussion}

\section{Conclusions}\sec{conclusions}

\bibliography{IEEE,references}

\end{document}
