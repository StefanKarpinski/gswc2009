\documentclass[conference]{IEEEtran}
\makeatletter
\def\ps@headings{%
\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}%
\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}%
\def\@oddfoot{}%
\def\@evenfoot{}}
\makeatother
\pagestyle{headings}

\newcommand{\thetitle}{On the Linear Structure of Network Traffic: Predicting Flow Behavior}

\input{packages}
\input{definitions}

\title{\vspace{-0.25em}\thetitle}
\author{
{\large{Stefan~Karpinski, John~R.~Gilbert, Elizabeth~M.~Belding}} \vspace{0.25em}\\
Department of Computer Science \\
University of California, Santa Barbara \vspace{0.35em}\\
\textit{\{sgk,gilbert,ebelding\}@cs.ucsb.edu}
}

\bibliographystyle{IEEEtran}

\newcommand{\figurename}{Figure}
\newcommand{\tablename}{Table}

\begin{document}
\maketitle

\begin{abstract}
We have observed that when network traffic behaviors are embedded into vector spaces in certain ways, they tend to exhibit a great deal of low-rank linear structure.
This observation leads us to the hypothesis that flow behaviors follow a specific type of finite mixture model.
Aside from being of theoretical interest, this model has practical consequences:
it allows us, among other things, to make detailed predictions about the probabilities of various future flow behaviors after observing only a handful of a flow's initial packets.
This practical application serves a dual function.
It provides a highly useful tool of network management, especially in wireless networks where such predictions can be leveraged to better allocate scarce bandwidth resources.
However, it also provides evidence that the hypothesized model not only explains the observed linear structure, but is an accurate model of real network traffic.
\end{abstract}

\section{Introduction}\sec{introduction}

\newfootnote{\flownote}{We use the common definition of a \textit{flow} as a sequence of packets sharing the same  ``5-tuple'': IP protocol type, source and destination nodes, and TCP/UDP port numbers.}

This paper is the first in a three-part series employing a numerical linear algebra techniques to understand and analyze network traffic patterns at a detailed level of individual flows and packets.\flownote
All three papers employ the same theoretical framework but investigate different fundamental applications of traffic modeling:
prediction, classification and generation.
In this paper we first present the framework and then demonstrate how it can predict the behavior of individual flows from the observation of only a handful of their initial packets.
% The following papers will focus on classification of flows according to application type and realistic workload generation.

The fundamental concept underlying this work is \emph{linear representation of network traffic}~\cite{Karpinski08}.
A linear representation is a function that maps sets of flows to vectors, the coordinates of which describe aspects of flow behavior;
it must distribute additively over set unions.
% the representation must also satisfy a simple linearity condition which we will describe in detail later.
% the sum of the representations of a set of flows is a vector that represents the aggregate behavior of the set of flows.
An entire traffic trace may be represented as a matrix whose rows are the representations of flows in the trace.
We restrict our consideration to a particularly natural and powerful class of representations whose coordinates are the frequencies of flow features in a set of flows.
For example, the representation can have a dimension for each packet size up to the maximum transfer unit (\caps{MTU});
coordinates in these dimensions indicate the count of packets of each size.
Port numbers, \caps{IP} protocol type, and inter-packet intervals can be handled similarly.
% Examples of flow features are the number of packets of a certain size or the use of a particular port number.
% The fundamental premise of this work, and of the sequel, is that the representatives of actual flow behaviors can be understood as being sampled from 

The essential observation of this research is that under such representations, real flows tend to lie near a small set of low-rank subspaces.
This property allows us to perform a drastic model reduction on trace traffic patterns while preserving the essential characteristics of the original traffic.
Moreover, when observable characteristics of flows are separated from features to be predicted, this model reduction is similar to a classical linear regression:
it allows us to use a least squares optimization on observed features to predict behavior.
We use this technique to predict the following characteristics of each flow from observing only a handful of its initial packets:
\begin{enumerate}
  \item the distribution of packets sizes,
  \item the distribution of inter-packet intervals,
  \item the number of packets that will be in the flow.
\end{enumerate}
From these properties we can indirectly predict many other properties, including the total duration and data volume.

At this point, it may be wise to address what we mean here by prediction.
Brief reflection shows that one cannot hope to accurately predict the exact behavior of flows:
flow behavior is nondeterministic---flows may exhibit the same initial behavior but subsequently behave quite differently.
The best we can do then is to predict a \emph{distribution} of possible outcomes given the observed initial behavior.
When compared with a specific outcome, such a prediction cannot be said to be ``wrong'' or ``right.''
How then can one measure the quality of a predicted distribution of outcomes?
This subtle and difficult issue is central to our evaluation methodology.

% real-world network traces tend to have approximately low-rank representation matrices despite the potential rank of such matrices being very large.
% Moreover, most flows' vectors lie on only one of a few even lower rank subspaces.
% Moreover, the low-rank subspace near which real traffic lies can be decomposed into subspaces of even lower rank, with almost all flows lying in one of these subspaces.
% Typically, these smaller subspaces are one-dimensional, but in some cases they may have up to a dozen dimensions.

% the vast majority of observed flow behaviors in any given trace lie in one of a handful low-rank subspaces.
% These subspaces are mostly of rank one, but a few have dimensions as high as ten.

% Karpinski~\emph{et~al.}~\cite{Karpinski08} further observed that the simplifications commonly used in traffic modeling correspond to multiplication of traffic representation matrices by low-rank matrices, averaging over rows or columns.
% They demonstrated experimentally that such simplifications severely distort important behavioral properties of traffic traces;
% viewing these simplification as low-rank matrix transformations, this is unsurprising:
% most of the detail of the original traffic behavior is destroyed.
% It was suggested that matrix factorization provides an alternative and superior means of model reduction.

\section{Background \& Motivation}\sec{background}\sec{motivation}

\begin{figure*}[t]
\vspace{-1em}
\begin{center}
\subfloat[\footnotesize{DARTMOUTH}]{\includegraphics[width=1.15in]{svd/dart}}
\subfloat[\footnotesize{IETF 60}]{\includegraphics[width=1.15in]{svd/ie60}}
\subfloat[\footnotesize{IETF 67}]{\includegraphics[width=1.15in]{svd/ie67}}
\subfloat[\footnotesize{SIGCOMM 2001}]{\includegraphics[width=1.15in]{svd/sc01}}
\subfloat[\footnotesize{SIGCOMM 2004}]{\includegraphics[width=1.15in]{svd/sc04}}
\subfloat[\footnotesize{UCSD}]{\includegraphics[width=1.15in]{svd/ucsd}}
\caption{SVD flow-behavior scatter plots for the six network traffic traces considered in this paper.} 
\fig{svd}
\end{center}
\vspace{-2em}
\end{figure*}

A linear representation, as proposed by Karpinski~\emph{et~al.}~\cite{Karpinski08}, maps sets of flows into a vector space such that disjoint union of sets becomes addition in the vector space:
\begin{align}
  \Phi(s_1 \disunion s_2) = \Phi(s_1) + \Phi(s_2).
\end{align}
The representation function should map each set of flows to a vector that in some sense describes their collective behavior.
For example: let $\Phi(s) \in \R^1$ be the number of packets in the flows in $s$.
Since the number of packets in the disjoint union of sets of flows is the sum of the number in each set individually, this representation is linear.
Given two representations, their direct sum is readily seen to also be a linear representation:
\begin{align}
  \Phi(s) = \Phi_1(s) \directsum \Phi_2(s).
\end{align}

A particularly powerful class of linear representations describes the behavior of collections of flows using histogram vectors to count the frequencies of various features.
A 1500-dimensional vector can be used to count the frequency of packet sizes seen, from one byte up to the typical \caps{MTU} of 1500 bytes.
In this representation, $\Phi(s)$ has as its $i$th entry the number of packets in $s$ of size $i$.
Since \caps{IP} protocols range from 0 to 255, the frequency of types in a set of flows can be represented in 256 dimensions.
Histogram representations for port numbers and inter-packet intervals are detailed in \Section{methodology}.
% Source and destination port numbers can similarly be represented using 65536-dimensional vectors.
% For practical purposes, however, this is too large a vector space, so we represent port numbers in 1000-dimensions, and reduce port numbers modulo 1000.
% Finally, we represent inter-packet intervals by first quantizing into 1500 bins of interval values as described by Karpinski~\emph{et~al.} and counting the number of intervals in flows occurring in each bin.
The representation we use is a direct sum of these histogram representations.
% The combination is a representation in 5256 dimensions.
An entire trace is represented as a matrix of such representation vectors---one flow per row.
This paper explores the linear structure of such matrices.

\newfootnote{\svdnote}{More precisely, the product of the first $k$ columns of $U$ with the first $k$ rows of $\trans{V}$ is a Frobenius-optimal rank-$k$ approximation of $X$.}

\newfootnote{\projectionnote}{We could normalize the vectors to unit length, but then our projection would be spherical, artificially introducing curvature into our visualization.}

To see the linear structure of these representation matrices, we apply one of the most powerful tools of linear algebra:
the singular value decomposition (\caps{SVD}).
This matrix decomposition can be applied to any real matrix, factoring it into a product of three real matrices:
\begin{align}
  X = US\trans{V}.
\end{align}
$U$ and $V$ are orthogonal and $S$ is diagonal, with entries of nonincreasing nonnegative value.
Among the useful properties of this factorization, is that the first $k$ columns of $US$ are an optimal $k$-dimensional reduced model of $X$.\svdnote
This property allows us to visualize the structure of $X$ using initial columns of $US$.
However, we are interested only interested in direction since
vectors differing only by magnitude lie in the same one-dimensional subspace.
To visualize direction while eliminating distance, we project all data points onto the unit-sum hyperplane from the origin.\projectionnote

\Figure{svd} illustrates the results of applying \caps{SVD} dimension reduction to sample matrices of six different traffic traces.
The figures show scatter plots of the most significant two projected \caps{SVD} coordinates for a matrix of 5000 sample flows from each trace.
% The original data encodes packet size distributions, inter-packet interval distributions, \caps{IP} protocol numbers, and port numbers for a sample of 5000 flows from each trace.
% The exact details of how the linear representation encodes these features will be discussed in \Section{methodology}.
Linear structure is visible in all six:
flow behaviors are heavily concentrated near low-dimensional subspaces.
Behaviors concentrated along a one-dimensional subspace appear as dark ``spots'' where many points have been plotted;
behaviors near two-dimensional subspaces appear as lines;
behaviors on higher-dimensional subspaces appear as ``smears.''
% There is a smattering of outliers, not especially near such structures.

The presence of linear structure in the matrix representations of several independent traffic traces immediately raises two significant questions.
The existential question: \emph{Why does this linear structure exist?}
Followed, of course, by the practical question: \emph{How can we exploit this structure?}
Neither question can be answered fully in a single paper, but we will begin to address the existential problem in the next section.
In the rest of the paper we apply this insight to the eminently practical problem of predicting the behavior of individual flows from the observation of only a few of their initial packets.

\section{Model \& Hypothesis}\sec{model-hypothesis}

Our answer to the question of why low-dimensional linear structure appears in the feature-frequency representation of network traffic can be expressed as a hypothetical model of flow behavior.
% A mixture model, to be specific.
We propose viewing each flow as having a probability distribution of possible behaviors it could exhibit.
% We propose that the behavior of each flow is drawn from a probability distribution on a space possible behaviors.
We hypothesize that the behavior distribution for most flows is a mixture of a relatively small set of ``basic behaviors.''
Moreover, only even smaller subsets of these basic behaviors are typically combined with each other.
Under these assumptions, we can express the distribution of each flow's behaviors as a finite mixture model~\cite{McLachlan00}:
\begin{align}\eqn{mixture-model}
  q_i(x) = \sum_{j=1}^r w_{ij} p_j(x),
\end{align}
Here $q_i$ and $p_j$ are probability density functions, and $w_{ij}$ are nonnegative weights, summing to unity for each $i$.
\Equation{mixture-model} is expressed succinctly as matrix multiplication.
Writing $Q_{ik} = q_i(k)$, $W_{ij} = w_{ij}$, and $P_{jk} = p_j(k)$, we have:
\begin{align}\eqn{mixture-model-matrix}
  Q = WP.
\end{align}
The number of basic behaviors, $r$, is the maximum possible rank of the feature distribution matrix, $Q$.
Moreover, we can partition the rows of $P$ into disjoint classes such that $w_{ij_1}$ and $w_{ij_2}$ are both non-zero only if $j_1$ and $j_2$ are in the same class.
Thus, each row of $Q$ is associated with exactly one class, and all the points associated with a class lie in the subspace spanned by its rows in $P$.

The properties of the model explain the structures seen in \Figure{svd}.
% This model is an ideal, and observed behaviors have noise introduced by sampling, imperfect adherence to the model, and outliers.
However, this is only one hypothesis that could fit the data.
Like any hypothesis, it must be tested.
The rest of this paper, aside from providing a practical application, serves as a hypothesis test:
we try to recover the matrices $W$ and $P$ from our noisy and imperfect observations of $Q$ and use the recovered model to predict real flow behaviors.
If the recovered model can make accurate predictions, this provides evidence that our model and hypothesis are valid.

\begin{table*}
\begin{center}
\small
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
\textbf{Trace} &
\textbf{Year} &
\textbf{Start} &
\textbf{End} &
\textbf{Type} &
\textbf{Network} \\
\hline

{\footnotesize{DARTMOUTH}} &
2003 &
Mon Nov  3 18:39:48 &
Wed Nov  5 16:09:48 &
campus &
Dartmouth College campus \\
\hline

{\footnotesize{IETF 60}} &
2004 &
Wed Aug  4 17:02:50 &
Wed Aug  4 18:02:49 &
conference &
IETF hotel venue \\
\hline

{\footnotesize{IETF 67}} &
2006 &
Wed Nov  8 19:31:56 &
Wed Nov  8 20:18:29 &
conference &
IETF hotel venue \\
\hline

{\footnotesize{SIGCOMM 2001}} &
2001 &
Wed Aug 29 17:24:47 &
Fri Aug 31 19:53:15 &
conference &
SIGCOMM hotel venue \\
\hline

{\footnotesize{SIGCOMM 2004}} &
2004 &
Tue Aug 31 13:25:04 &
Fri Sep  3 21:40:22 &
conference &
SIGCOMM hotel venue \\
\hline

{\footnotesize{UCSD}} &
2007 &
Thu Jan 11 07:59:50 &
Fri Jan 12 07:59:55 &
campus &
UCSD engineering building \\
\hline

\end{tabular}
\caption{Traffic traces used for analysis and experiments.}
\tab{traces}
\end{center}
\vspace{-2em}
\end{table*}

\section{Methodology}\sec{methodology}

Our experimental methodology has three parts:
training, prediction and evaluation.
For training, we take a sample of network trace traffic and try to recover the structure hypothesized in \Section{model-hypothesis} from it.
For prediction, we use the trained model to predict behaviors of a separate set of flows from the same trace, using only knowledge of a few initial packets of each.
Finally, we must evaluate the quality of these predictions.
As noted in the \Section{introduction}, due to nondeterminism we cannot possibly predict exact behaviors of flows and must instead evaluate how good a predicted distribution of possible behaviors is as compared to actual outcomes.
% One relatively intuitive measure of prediction quality we use is threshold testing.

% Our methodology for recovering the hypothesized linear structure of network traffic from trace samples uses a variety of powerful computational tools:
% \begin{enumerate}
%   \item Compute the linear representation matrix of the trace.
%   \item Perform linear subspace segmentation, extracting subspaces near which significant amounts of data points lie.
%   % For this step, we use Ma~\emph{et~al.}'s 2007 algorithm based on lossy compression of data~\cite{Ma07}.
%   \item Use \caps{SVD} to determine an approximation rank for each segmentation and an optimal approximation basis.
%   \item Compute the vertices of the intersection of each approximation subspace with the standard simplex.
%   For each subspace, all nonnegative points can be written as convex linear combinations of these vertices.
%   % For this we use Fukuda's excellent \caps{CDD}lib software for high-dimensional polyhedral computations, implementing a variety of convex hull and vertex enumeration algorithms~\cite{Avis92}.
%   \item Find optimal weights to approximate each data point from the vertices of its subspace.
% \end{enumerate}

\subsection{Data Sets}

We use randomly sampled traffic from six different network traces.
All six traces are freely available from the \caps{CRAWDAD} wireless trace repository~\cite{Yeo06}.
These traces represent a cross-section of traffic patterns over time and a reasonable variety of network types.
Unfortunately, we could find no freely available commercial hotspot or corporate network traces that were usable for our analysis.
We randomly sampled 5000 flows from each trace for training and another 5000 flows for testing.

\subsection{Feature-Frequency Representation}\sec{representation}

We embed the following features of flow behavior into vector spaces via feature-frequency linear representation:
packet size distribution,
inter-packet interval distribution,
packet count,
\caps{IP} protocol type,
\caps{TCP/UDP} port numbers.
All features are represented using frequency histograms.
For packet size distribution, this is trivial since packet sizes are discrete, taking on only values from 1 to the \caps{MTU}, which was 1500 in all of our traces.
Inter-packet intervals must be quantized, using the scheme described by Karpinski~\emph{et~al.}~\cite{Karpinski08} but using only 500 dimensions instead of 1500.
This scheme maps bins of interval durations to indices, with the bins starting at the scale of milliseconds, and ramping smoothly up to seconds at the high end of the scale.
Intervals of more than ten minutes are considered to start a new flow.

\caps{IP} protocol type is naturally discrete, with values from 0 to 255, represented by a 256-dimensions histogram vector.
The ``histogram'' for a single flow is always just an indicator vector with a single unit entry.
However, the linear embedding of this feature into a frequency space allows us to infer the distribution of possible values using linear analysis.
Similarly, port numbers are naturally discrete.
However, the range of port numbers (0-65535) is too large for practical representation using a full histogram.
Instead, we represent port numbers by their remainder modulo 1000.
Port numbers are sufficiently spread out that this still provides ample distinction between traffic types.
We also represent the packet count of each flow.
Since the potential packet count is unlimited, we must take some approach to limiting the dimension of this frequency representation.
We use the simplest approach and simply cap the packet count at 3000 packets:
all flows with 3000 or more packets fall into the same histogram bin.
In practice, flows longer than 3000 packets are ``long.''

Each flow is thus represented as a direct sum of frequency vectors:
\begin{math}
  \text{size} \directsum
  \text{ival} \directsum
  \text{type} \directsum
  \text{ports} \directsum
  \text{pkts} \in \R^{6256}.
\end{math}
A collection of flows is represented as a ``stacked'' matrix of row vectors:
\begin{align}
  X = [ ~
  \text{Size} ~
  \text{Ival} ~
  \text{Type} ~
  \text{Ports} ~
  \text{Pkts} ~
  ] \in \R^{m \times 6256}.
\end{align}

\subsection{Subspace Segmentation}\sec{subspace-segmentation}

\subsection{Subspace Factorization}\sec{subspace-factorization}

\subsection{Flow Behavior Prediction}\sec{flow-behavior-prediction}

\subsection{Evaluating Predictions}\sec{evaluation}

\section{Results}\sec{results}

\begin{figure*}[t]
\begin{center}
\subfloat[Packet sizes: \footnotesize{DARTMOUTH}]{%
\includegraphics[width=3.545in]{size/dart}}
\subfloat[Packet sizes: \footnotesize{SIGCOMM 2004}]{%
\includegraphics[width=3.545in]{size/sc04}}
\subfloat[Inter-packet intervals: \footnotesize{DARTMOUTH}]{%
\includegraphics[width=3.545in]{ival/dart}}
\subfloat[Inter-packet intervals: \footnotesize{SIGCOMM 2004}]{%
\includegraphics[width=3.545in]{ival/sc04}}
\caption{%
Empirical CDFs of Kolmogorov-Smirnov p-values for various predictors of packet size distribution and inter-packet interval distribution for two selected traces. Lower curves are better and the ideal curve provides a lower bound on quality.
Vertical segments indicate the farthest distance between the ideal and each CDF.
This distance serves as a pessimistic measure of each predictor's error rate.
}
\fig{cdf}
\end{center}
\vspace{-2em}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\hfill
\subfloat[Packet count: accuracy]{%
\includegraphics[width=2.1in]{pkts_accuracy}}\hfill
\subfloat[Packet count: uncertainty reduction]{%
\includegraphics[width=2.1in]{pkts_certainty}}\hfill
\subfloat[Packet count: prediction entropy]{%
\includegraphics[width=2.1in]{pkts_entropy}}\hfill
\caption{Packet count prediction accuracy, uncertainty reduction and entropy.}
\fig{packets-accuracy}
\end{center}
\vspace{-2em}
\end{figure*}

\section{Discussion}\sec{discussion}

\section{Conclusions}\sec{conclusions}

\bibliography{IEEE,references}

\end{document}
