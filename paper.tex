\documentclass[conference]{IEEEtran}
\makeatletter
\def\ps@headings{%
\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}%
\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}%
\def\@oddfoot{}%
\def\@evenfoot{}}
\makeatother
\pagestyle{headings}

\newcommand{\thetitle}{On the Linear Structure of Network Traffic: Predicting Flow Behavior}

\input{packages}
\input{definitions}

\title{\vspace{-0.25em}\thetitle}
\author{
{\large{Stefan~Karpinski, John~R.~Gilbert, Elizabeth~M.~Belding}} \vspace{0.25em}\\
Department of Computer Science \\
University of California, Santa Barbara \vspace{0.35em}\\
\textit{\{sgk,gilbert,ebelding\}@cs.ucsb.edu}
}

\bibliographystyle{IEEEtran}

\newcommand{\figurename}{Figure}
\newcommand{\tablename}{Table}

\begin{document}
\maketitle

\begin{abstract}
We have observed that when network traffic behaviors are represented in vector spaces as relative frequency histograms of behavioral features, they exhibit low-rank linear structure.
We hypothesize that this structure is due the distribution of flow behaviors following a  finite mixture model.
Aside from being of theoretical interest, this hypothesis has practical consequences:
it allows us, among other things, to make detailed, accurate predictions about the probabilities of various future flow behaviors after observing only a handful of a flow's initial packets.
We are able to predict the distribution of future packet sizes and inter-packet intervals from only five initial packets with between 70\% and 90\% accuracy across a variety of network traces;
for about half of flows, we can reduce uncertainty about packet count by 80\% or more.
These practical applications serve dual functions.
They provide highly useful tools for network management, routing decisions, and quality of service schemes.
However, they also provide evidence that the hypothesized model gives a correct explanation for the observed linear structure in real network traffic.
\end{abstract}

\section{Introduction}\sec{introduction}

\newfootnote{\flownote}{We use the common definition of a \textit{flow} as a sequence of packets sharing the same  ``5-tuple'': IP protocol type, source and destination nodes, and TCP/UDP port numbers.}

This paper is the first in a three-part series employing a numerical linear algebra techniques to understand and analyze network traffic patterns at a detailed level of individual flows and packets.\flownote
All three papers employ the same analytical approach but investigate different fundamental applications of traffic modeling:
prediction, classification and generation.
In this paper we present our hypothesis and demonstrate how it can be used to predict the behavior of individual flows from observing only a handful of initial packets.
% The following papers will focus on classification of flows according to application type and realistic workload generation.

This work begins with a particular way of representing flow behaviors as vectors.
% and the behaviors of entire traffic traces as large, sparse matrices.
% This approach was first proposed in by Karpinski~\emph{et~al.}~\cite{Karpinski08}, who called this general type of representation \emph{linear representations}.
The representation is quite simple.
For each feature of a flow, we represent that aspect of the flow's behavior as a \emph{feature-frequency vector}:
a vector having a dimension for each possible value of the feature and whose coordinates are the relative frequency of values.
For example, the vector representing the distribution of packet sizes of a flow having four 40-byte and two 145-bytes packets is
\begin{align}
  \text{size} = \frac{1}{6}\parens{4\vec{e}_{40} + 2\vec{e}_{145}}.
\end{align}
% The dimension of this representation is 1500 because that is typically the maximum transfer unit (\caps{MTU}) of local networks.
Different aspects of flow behavior can be represented in this way, and these representations can be combined by taking the direct sum of their representation vectors.

The behavior of any set of flows, such as a traffic trace, can be represented as a matrix of feature-frequency vectors.
We use the convection that features values correspond to matrix columns while flows correspond to matrix rows.
When traffic traces are represented like this, a very curious thing happens:
the resulting matrices exhibit a great deal of linear structure.
Specifically, flow behaviors tend to lie near the union of a small set of low-rank subspaces.
It is the investigation of this linear structure that lies at the core of this paper.

% We hypothesize that the reason for the linear structure is that the distribution of each flows behavior is a finite mixture of a small set of ``basic behaviors.''
% Moreover, the mixing of basic behaviors is itself structured.
The linear structure of network traffic allows us to perform a drastic model reduction on trace traffic patterns while preserving the essential characteristics of the original trace.
Furthermore, when observable characteristics of flows are separated from features to be predicted, this model reduction is similar to a classical linear regression:
it allows us to use least squares optimization on observed features to predict individual flow behavior.
We use this technique to predict the following characteristics of flows from a few initial packets:
\begin{enumerate}
  \item the distribution of packets sizes,
  \item the distribution of inter-packet intervals,
  \item the number of packets that will be in the flow.
\end{enumerate}
From these properties we can indirectly predict many other properties, including the total duration and data volume.

This prediction technique is clearly of immediate practical use:
applications range from network management to routing to quality of service.
Looking deeper than these applications, however, the mere fact that this prediction technique works at all provides evidence that the underlying hypothesized model of flow behavior is valid.
Our future works will provide evidence and applications by applying this model to the problems of traffic classification and realistic workload generation.

The rest of this paper is organized as follows.
In \Section{background}, we provide some background and the present the observations which motivate this work.
Our model and hypothesis about flow behavior is presented in \Section{hypothesis}.
The experimental methodology for testing this hypothesis is presented in \Section{methodology} while the results follow in \Section{results}.
\Section{discussion} discusses the meaning and impact of these results and
\Section{related-work} gives context for this research in relation prior work.
We conclude with final remarks in \Section{conclusions}.

\section{Background \& Motivation}\sec{background}\sec{motivation}

\begin{figure*}[t]
\vspace{-1em}
\begin{center}
\subfloat[\footnotesize{DARTMOUTH}]{\includegraphics[width=1.15in]{svd/dart}}
\subfloat[\footnotesize{IETF 60}]{\includegraphics[width=1.15in]{svd/ie60}}
\subfloat[\footnotesize{IETF 67}]{\includegraphics[width=1.15in]{svd/ie67}}
\subfloat[\footnotesize{SIGCOMM 2001}]{\includegraphics[width=1.15in]{svd/sc01}}
\subfloat[\footnotesize{SIGCOMM 2004}]{\includegraphics[width=1.15in]{svd/sc04}}
\subfloat[\footnotesize{UCSD}]{\includegraphics[width=1.15in]{svd/ucsd}}
\caption{SVD flow-behavior scatter plots for the six network traffic traces considered in this paper.} 
\fig{svd}
\end{center}
\vspace{-2em}
\end{figure*}

\newfootnote{\directsumnote}{The direct sum is the vector space analogue of a cross product of sets.}

Before we can present our motivating observation about the linear structure of network traffic, we must explain how flow behaviors are represented as feature-frequency vectors. The low-level details of this representation are given in \Section{representation}.
For now, what the reader must know is that each feature is represented by a vector where each dimension corresponds to a possible value of the feature;
the coordinates of the vector are the normalized frequencies of the corresponding feature values.
We have already given the example of packet size distribution in the introduction.
Vectors are normalized so the coordinates of each feature-frequency vector have unit sum.
To combine multiple feature-frequency vectors into a composite description of the overall behavior of a flow, we take the direct sum individual feature representations:\directsumnote
\begin{align}\eqn{flow}
  \text{flow} =
  \text{size} \directsum
  \text{ival} \directsum
  \text{type} \directsum
  \text{port} \directsum
  \text{pkts}.
\end{align}
The behavior of a feature across a collection of flows can be expressed as a matrix where each row represents a flow:
\begin{align}\eqn{Size}
  \text{Size} = \begin{bmatrix}
    \text{size}_1 \\
    \vdots \\
    \text{size}_m
  \end{bmatrix}.
\end{align}
The overall behavior of the collection of flows then becomes a concatenation of these feature matrices:
\begin{align}\eqn{X}
  X = \bracket{ ~
    \text{Size} ~
    \text{Ival} ~
    \text{Type} ~
    \text{Port} ~
    \text{Pkts} ~
  }.
\end{align}
It is this matrix, representing the total behavior of a collection of flows, to which we apply our analysis.

\newfootnote{\svdnote}{More precisely, the product of the first $k$ columns of $US$ with the first $k$ rows of $\trans{V}$ is a Frobenius-optimal rank-$k$ approximation of $X$.}

\newfootnote{\projectionnote}{We could normalize the vectors to unit length, but then our projection would be spherical, artificially introducing curvature into our visualization.}

To see the linear structure of these representation matrices, we apply one of the most fundamental tools of linear algebra:
the singular value decomposition (\caps{SVD}).
This matrix decomposition can be applied to any real matrix, factoring it into a product of three real matrices:
\begin{align}
  X = US\trans{V}.
\end{align}
$U$ and $V$ are orthogonal and $S$ is diagonal, with entries of nonincreasing nonnegative value.
Among the useful properties of this factorization, is that the first $k$ columns of $US$ are an optimal $k$-dimensional reduced model of $X$.\svdnote
This property allows us to visualize the structure of $X$ using initial columns of $US$---and this is precisely what we do.
However, it must be done in a particular way.
First determine an appropriate value for $k$.
We use the number of dimensions necessary to account for 75\% of the variance seen in $X$;
other approaches work well too.
Discarding the rest of the dimensions has the effect of ``denoising'' the data.
The vectors of the remaining $k$-dimensional reduction of $X$ may have different magnitudes, but we are only interested in direction, since vectors differing only by magnitude lie in the same one-dimensional subspace.
Therefore, we project each row vector through the origin onto the unit-sum hyperplane.\projectionnote
It is the coordinates of this reduced, projected data which exhibit linear structure.

\Figure{svd} shows scatter plots of most significant two coordinates when this procedure is applied to a random sample of 5000 flows from each of six independent traffic traces.
Clear linear structure is visible in all six traces:
flow behaviors are heavily concentrated near low-dimensional subspaces.
Behaviors concentrated along a one-dimensional subspace appear as dark ``spots'' where many points have been plotted;
behaviors near two-dimensional subspaces appear as lines;
behaviors on higher-dimensional subspaces appear as ``smears.''

The presence of linear structure in the matrix representations of several independent traffic traces immediately raises two significant questions.
The existential question: \emph{Why does this linear structure exist?}
Followed, of course, by the practical question: \emph{How can we exploit this structure?}
Neither question can be answered fully in a single paper, but we will attempt to address the existential problem in the next section.
In the rest of this paper we apply this insight to the eminently practical problem of predicting the behavior of individual flows from the observation of only a few of their initial packets.

\section{Model \& Hypothesis}\sec{model}\sec{hypothesis}

\begin{table*}
\begin{center}
\small
\begin{tabular}{|c|c|c|c|c|c|c|}

\hline
\textbf{Trace} &
\textbf{Year} &
\textbf{Start} &
\textbf{End} &
\textbf{Type} &
\textbf{Network} \\
\hline

{\footnotesize{DARTMOUTH}} &
2003 &
Mon Nov  3 18:39:48 &
Wed Nov  5 16:09:48 &
campus &
Dartmouth College campus \\
\hline

{\footnotesize{IETF 60}} &
2004 &
Wed Aug  4 17:02:50 &
Wed Aug  4 18:02:49 &
conference &
IETF hotel venue \\
\hline

{\footnotesize{IETF 67}} &
2006 &
Wed Nov  8 19:31:56 &
Wed Nov  8 20:18:29 &
conference &
IETF hotel venue \\
\hline

{\footnotesize{SIGCOMM 2001}} &
2001 &
Wed Aug 29 17:24:47 &
Fri Aug 31 19:53:15 &
conference &
SIGCOMM hotel venue \\
\hline

{\footnotesize{SIGCOMM 2004}} &
2004 &
Tue Aug 31 13:25:04 &
Fri Sep  3 21:40:22 &
conference &
SIGCOMM hotel venue \\
\hline

{\footnotesize{UCSD}} &
2007 &
Thu Jan 11 07:59:50 &
Fri Jan 12 07:59:55 &
campus &
UCSD engineering building \\
\hline

\end{tabular}
\caption{Traffic traces used for analysis and experiments.}
\tab{traces}
\end{center}
\vspace{-2em}
\end{table*}

Our answer to the question of why low-dimensional linear structure appears in the feature-frequency representation of network traffic can be expressed as the hypothesis that flow behavior follows a specific statistical model.
% A mixture model, to be specific.
We propose viewing each flow as having a probability distribution of possible behaviors it could exhibit.
% We propose that the behavior of each flow is drawn from a probability distribution on a space possible behaviors.
We hypothesize that the behavior distribution for most flows is a mixture of a relatively small set of ``basic behaviors.''
Moreover, only even smaller subsets of these basic behaviors are typically combined with each other.
Under these assumptions, we can express the distribution of each flow's behaviors as a finite mixture model~\cite{McLachlan00}:
\begin{align}\eqn{mixture-model}
  q_i(x) = \sum_{j=1}^r w_{ij} p_j(x),
\end{align}
Here $q_i$ and $p_j$ are probability density functions, and $w_{ij}$ are nonnegative weights, summing to unity for each $i$.
\Equation{mixture-model} is expressed succinctly as matrix multiplication.
Writing $Q_{ik} = q_i(k)$, $W_{ij} = w_{ij}$, and $P_{jk} = p_j(k)$, we have:
\begin{align}\eqn{mixture-model-matrix}
  Q = WP.
\end{align}
The number of basic behaviors, $r$, is the maximum possible rank of the feature distribution matrix, $Q$.
Moreover, we can partition the rows of $P$ into classes such that $w_{ij_1}$ and $w_{ij_2}$ are both non-zero only if $j_1$ and $j_2$ are in the same class.
Thus, each row of $Q$ is associated with exactly one class, and all the points associated with a class lie in the subspace spanned by its associated rows in $P$.
Of course, even if this model is true, actual observed behaviors will be sampled from the ideal distributions of $Q$, producing an observation matrix, $X$, with a great deal of noise.
% In particular, for features with vary small samples, the amount of noise introduced is quite large.
Moreover, we expect that there should be many outliers:
network traffic is such a complex phenomenon that no single model will explain all observations.

Despite these caveats, this model explains the structures found in \Figure{svd}.
Points along the same low-rank structure are in the same class.
Each structure is ``generated'' by a small set of vertices:
the points belonging to the structure are near the hull of its vertices.
However, this is only one possible hypothesis that fits the data.
Like any hypothesis, it must be tested.
The rest of this paper, aside from providing a practical application, serves as a hypothesis test:
we try to recover the matrices $W$ and $P$ from our noisy and imperfect observations of $Q$ and use the recovered model to predict real flow behaviors.
If the recovered model can make accurate predictions, this provides evidence that our model and hypothesis are valid.

\section{Methodology}\sec{methodology}

Our experimental procedure has three parts:
training, prediction and evaluation.
For training, we take samples of network traces and attempt to recover the parameters of the model hypothesized in \Section{hypothesis}.
We use the recovered model to predict behaviors of a separate set of flows from the same trace, using only knowledge of five initial packets from each flow.
Finally, we must evaluate the quality of these predictions by comparing them against the actual behavior of those flows.
In this section we present the methodology for training and prediction;
how to evaluate prediction results is described together with the results in \Section{results}.

\subsection{Data Sets}

We use randomly sampled traffic from six different network traces.
All six traces are freely available from the \caps{CRAWDAD} wireless trace repository~\cite{Yeo06}.
Details of the traces are shown in \Table{traces}.
These traces represent a broad cross-section of traffic patterns over time and a reasonable variety of network types.
Unfortunately, we could not find any freely available residential or corporate network traces that were usable for our analysis.
We randomly sampled 5000 flows from each trace for training and another 5000 flows for testing.
Since the initial five packets are used for prediction, we consider only flows with at least five packets.

\subsection{Feature-Frequency Representation}\sec{representation}

We embed the following features of flow behavior into vector spaces via feature-frequency representation:
packet size distribution,
inter-packet interval distribution,
\caps{IP} protocol type,
\caps{TCP/UDP} port numbers,
and packet count.
%
Packet sizes are naturally discrete, taking on values from 1 to the \caps{MTU}, which was 1500 in all of our traces.
%
Inter-packet intervals must be quantized, using the scheme described by Karpinski~\emph{et~al.}~\cite{Karpinski08} but using only 500 dimensions instead of 1500.
This scheme maps bins of interval durations to indices, with the bins starting at the scale of milliseconds, but ramping smoothly up to seconds at the high end of the scale.
Intervals of more than ten minutes are considered to start a new flow.
%
\caps{IP} protocol type is also naturally discrete, with values from 0 to 255, represented by a 256-dimensional vector.
The feature-frequency vector for a single flow is always just a vector with a single unit entry and zeros elsewhere.
%
Port numbers are naturally discrete as well, but the range of port numbers (0-65535) is too large for practical representation using a full histogram.
Instead, we represent port numbers by their remainder modulo 1000.
Port numbers are sufficiently spread out that this still provides ample distinction between traffic types.
Moreover, since the direction of a flow is generally uninteresting, we represent both source and destination in a single vector containing two values.
%
We also represent the packet count of each flow.
Since the potential packet count is unlimited, some approach is needed for limiting the dimension of this frequency representation.
We use the simplest approach and simply cap the packet count at 3000 packets:
all flows with 3000 or more packets fall into the same histogram bin and are simply considered ``very large.''

\subsection{Model Recovery}\sec{model-recovery}

\subsubsection{Subspace Segmentation}\sec{subspace-segmentation}

Once the data have been expressed in matrix form, the next step is try to detect the linear structures seen in \Figure{svd}.
% Recall that these correspond to the point classes in our hypothesized model.
We use Ma~\emph{et~al.}'s algorithm for segmenting multivariate data into a union of subspaces using lossy data coding and compression~\cite{Ma07}.
The essential idea of this algorithm is to compute the number of bits needed to encode the data under various groupings.
The grouping which yields the best compression is the desired segmentation.
This algorithm has several desirable qualities.
Two are essential for us:
1) unlike most clustering algorithms, it finds subspaces, not regions;
2) it is highly robust to outliers and noisy data.
The algorithm has one parameter:
the maximum allowable distortion of the data.
We follow the authors when applying the algorithm to \caps{SVD} data and use $\varepsilon^2 = 1$.
The result of this stage is a segmentation of the $q_i$ into classes associated with structures, precisely as our hypothesized model requires.

\subsubsection{Subspace Factorization}\sec{subspace-factorization}

\newfootnote{\svdnnnote}{The first singular vector of a nonnegative matrix is guaranteed to be nonnegative, but the other singular vectors will have mixed sign.}

The next step towards recovering the structure of the hypothesized model is to determine the hull points of each linear structure.
In the previous step, we have determined, as well as we can, which $q_i$ belong to the same structures.
Now we must determine the vertices, $p_j$, which generate these structures.
We can use \caps{SVD} to determine the most prominent linear components of each structure, but \caps{SVD} will give us vectors of mixed sign.\svdnnnote
Since negative probability distributions are meaningless, these do not give us a plausible recovery of our model.
We must turn to other techniques to recover possible vectors for the $p_j$.

In essence, what we have is a nonnegative matrix factorization (\caps{NMF}) problem:
if $Q_*$ is a sub-matrix of rows in the same structure class, we want to find nonnegative matrices, $W_*$ and $P_*$, such that $Q_* \approx W_* P_*$.
% We have the additional constraint that $W_*$ and $P_*$ have unit-sum rows, but that can easily be guaranteed since 
Large $P$ is a vertical concatenation of these $P_*$ matrices, while $W$ is row-permutation of the direct sum of $W_*$ matrices.
Many \caps{NMF} algorithms have been proposed since Lee and Seung published the first~\cite{Lee01}.
We use Kim and Park's alternating non-negative least squares (\caps{ANLS}) algorithm~\cite{Kim08:anls} first for its rapid initial convergence, but refine the result using Lee and Seung's Euclidean algorithm.

\newfootnote{\cddnote}{To do this, we use Avis and Fukuda's vertex enumeration algorithm as implemented in Fukuda's excellent CDD library~\cite{Avis92}.}

We find that using the standard random \caps{NMF} initialization does not yield factorizations of sufficient quality, so we have developed the following initialization procedure.
Reduce the data to the number structural dimensions sought using \caps{SVD}.
Cluster the reduced data points into one cluster per structural dimension using $k$-means clustering.
Use cluster membership from this step to combine the original data, leaving only one composite row per desired structural dimension.
These rows provide a ``denoised'' set of nonnegative spanning vectors that estimate the true low-dimensional subspace from which the data were sampled.
Finally, we find the intersection of this subspace with the standard simplex in $\R^n$.\cddnote
This step gives us a set of nonnegative vertices geometrically guaranteed to contain all of the estimated low-dimensional subspace.
These vectors are used to initialize the \caps{NMF} algorithms, yielding extremely good factorization for small structures.

\subsection{Flow Behavior Prediction}\sec{flow-behavior-prediction}

To predict flow behavior, we must separate flow features into those that are observed and those that must be predicted.
For example, if we want to predict total flow behavior from some number of initial packets, then we would have the following matrix of observables:
\begin{align}
  X_\text{o} = \bracket{ ~
    \text{Size}_\text{init} ~
    \text{Ival}_\text{init} ~
    \text{Type} ~
    \text{Port} ~
  }
\end{align}
Observing five initial packets, $\text{Size}_\text{init}$ is the packet size feature-frequency vector for only those five packets. Similarly, $\text{Ival}_\text{init}$ is the feature-frequency vector for the first four inter-packet intervals.
The matrix of ``predictables'' would be:
\begin{align}
  X_\text{p} = \bracket{ ~
    \text{Size}_\text{rest} ~
    \text{Ival}_\text{rest} ~
    \text{Pkts} ~
  }
\end{align}
The matrices $\text{Size}_\text{rest}$ and $\text{Ival}_\text{rest}$ are the analogous  matrices for the rest of the packets in the flows.

From the observed data, $X_\text{o}$, we find the best fitting weight matrix, and use the weights together with our recovered model parameters to estimate probability distributions of features for each flow, using the hypothesized model.
This technique is fundamentally similar to fitting a regression line to noisy two-dimensional training data and then using the $x$-coordinates of data points to linearly predict the $y$-coordinates.
The main differences are the vastly higher number of dimensions and more complex structure we assume for our data.

% We use the packet sizes and inter-packet intervals of the initial packets, together with their \caps{IP} type and port numbers to predict the distribution of packets sizes and inter-packet intervals for the rest of the flow, as well as predicting a distribution of possible of packet counts.
% In order to do this prediction, subspace factorization must be done with the observable and predictable matrices separated.
% That is, subspace factorization must be done on the matrix $[X_\text{o}~X_\text{p}]$.
% However, we perform our subspace segmentation step on the combined data as in \Equation{X}.
% Why the difference? We have two pragmatic reasons:
% 1) the segmentation step seems to work better with the unseparated matrix.
% 2) we want to apply our prediction technique with different numbers of initial packets;
% the segmentation step is very slow, while the factorization step is relatively fast;
% this way we can use the same segmentation result for varying numbers of initial packets.

\newfootnote{\separationnote}{%
We use an unseparated feature matrix for subspace segmentation, but separated observable and predictable features for subspace factorization.
This is partly because segmentation appears to work better using combined features, but also so that we can reuse the same expensive segmentation computation when performing prediction with different numbers of initial packets.
}

Let $P^* = \bracketx{P^*_\text{o}~P^*_\text{p}}$ be the recovered model parameter matrix from ~\Section{model-recovery}, with separated observable and predictable features.\separationnote
From an observation matrix, $X_\text{o}$, we estimate the matrix of weights by minimizing the squared Frobenius error:
\begin{align}
  W^* = \text{argmin}_W {\norm{X_\text{o} - W P^*_\text{o}}^2_\text{frob}}.
\end{align}
Note that since the weights must be nonnegative, a nonnegative least squares algorithm must be used~\cite{Benthem04,Kim08:block-pivot}.
From here, we can estimate the underlying feature distributions for the flows:
\begin{align}
  Q^* = W^*P.
\end{align}
In particular, the ``predictable'' portion, $Q^*_\text{p}$, contains predictions of packet size distribution, inter-packet interval distribution and distribution of packet counts for each flow.
To evaluate the quality of these predictions, we compare the distributions in $Q^*_\text{p}$ to the matrix, $X_\text{p}$, of actual behaviors of flows.
The next section explores how to preform this evaluation and how our technique performs compared with other possible predictors. 

\section{Results}\sec{results}

\begin{figure*}[t]
\vspace{-1em}
\begin{center}
\subfloat[Packet sizes: \footnotesize{DARTMOUTH}]{%
\includegraphics[width=2.35in]{size/dart}}
\subfloat[Packet sizes: \footnotesize{IETF 67}]{%
\includegraphics[width=2.35in]{size/ie67}}
\subfloat[Packet sizes: \footnotesize{SIGCOMM 2004}]{%
\includegraphics[width=2.35in]{size/sc04}}
\subfloat[Inter-packet intervals: \footnotesize{DARTMOUTH}]{%
\includegraphics[width=2.35in]{ival/dart}}
\subfloat[Inter-packet intervals: \footnotesize{IETF 67}]{%
\includegraphics[width=2.35in]{ival/ie67}}
\subfloat[Inter-packet intervals: \footnotesize{SIGCOMM 2004}]{%
\includegraphics[width=2.35in]{ival/sc04}}
\caption{%
Empirical CDFs of Kolmogorov-Smirnov p-values for various predictors of packet size distribution and inter-packet interval distribution for two selected traces. Lower curves are better and the ideal curve provides a lower bound on quality.
Vertical segments indicate the farthest distance between the ideal and each CDF.
This distance serves as a measure of each predictor's overall error rate.
}
\fig{cdf}
\end{center}
\vspace{-1.5em}
\end{figure*}

% It seems wise at this point to address the nature of prediction for highly nondeterministic phenomena like network traffic.
Brief reflection shows that one cannot hope to accurately predict the exact behavior of flows:
flows may exhibit the same initial behavior but subsequently behave quite differently.
The best we can hope to do is to predict a \emph{distribution} of possible outcomes given the observed initial behavior.
When compared with a specific outcome, such a prediction cannot be said to be unequivocally right or wrong.
% How then can we measure the quality of a predicted distribution of outcomes as compared to a single outcome?
% This subtle and difficult issue is central to our evaluation methodology.
% In what follows, we have performed prediction from observing only five initial packets from each flow.

% To compare the predicted distributions of packet sizes and inter-packet intervals against the observed distributions, we use the most common ``goodness-of-fit'' test:

To test the quality of a predicted distribution as compared with a sample of values which may or may not come from that distribution, we use the single-sample Kolmogorov-Smirnov (K-S) ``goodness-of-fit'' test~\cite{Feller68}.
The K-S test compares the cumulative distribution function (\caps{CDF}) of a theoretical distribution with the empirical cumulative distribution function of the sample.
The test uses the largest absolute difference between the \caps{CDF}s as a statistic to measure how plausible it is that the sample was drawn from the hypothesized distribution.
The K-S statistic is transformed into a p-value:
the probability of obtaining a result as extreme as observed, assuming the sample really came from the hypothesized distribution.

\newfootnote{\ksidealnote}{If the K-S test were perfect, the pragmatic ideal baseline would be uniform. The fact that it is not demonstrates the imperfection of the K-S test.}

We evaluate the quality of our packet size and inter-packet interval distribution predictions by examining the distribution of K-S p-values for the predicted distribution versus the observed sample.
In theory, if the predictions are correct, the p-values should be uniformly distributed on the interval [0,1].
In practice, however, the K-S test is not perfect, and its p-values are not uniform.
Instead of comparing predictions to the nonexistent theoretical ideal, we use a more pragmatic ideal as a baseline:
p-values of samples that are actually drawn from the predicted distributions.
We measure quality of predictions by how much worse that this ideal the predictions are.\ksidealnote

\Figure{cdf} shows selected p-value \caps{CDF}s for our predicted distributions compared to the ideal baseline.
For comparison, we also show the results for two simplistic prediction techniques.
The first alternative, ``mean,'' always predicts the average behavior of the entire training trace.
This does not perform as badly as one might think it would:
always guessing the average is not an abysmal failure.
The second alternative, ``self,'' predicts the behavior that has already been seen.
% This is the best guess one can make about a flow, without having been trained on any other flows:
% just guess that it will keep doing what it has already done.

A na\"ive point-measure of the error of these predictions is the rate at which they fail the K-S test at a fixed significance level, such as 1\% or 5\%, as compared to the ideal.
This is equivalent to measuring the distance above the ideal \caps{CDF} at the 0.01 or 0.05 p-value mark.
This measure of error is na\"ive because these traditional significance levels are intentionally highly conservative:
they are chosen so as to incorrectly reject the null only 1\% or 5\% of the time.
Thus, they only reject the worst possible predictions.
The mean predictor makes a perfect example:
by guessing the average behavior it actually makes relatively few really bad predictions but makes far more poor and mediocre predictions than it should, and very few good predictions.
Our predictor, on the other hand, maintains a fairly steady error rate somewhat above the ideal, and in particular makes as many near-perfect predictions as the ideal.

\begin{figure*}[!t]
\vspace{-1em}
\begin{center}
\subfloat[CDF of accuracy p-values]{\fig{packets-accuracy}%
\includegraphics[width=2.351in]{pkts_accuracy}}
\subfloat[CDF of prediction entropy values]{\fig{packets-entropy}%
\includegraphics[width=2.351in]{pkts_entropy}}
\subfloat[CDF of reduction of uncertainty]{\fig{packets-uncertainty}%
\includegraphics[width=2.351in]{pkts_certainty}}
\caption{%
CDFs displaying the accuracy, uncertainty reduction, and entropy of our packet count predictions.
Ideal behavior in \Figure{packets-accuracy} is as close to the diagonal as possible.
For \Figure{packets-entropy}, lower entropy is better.
the vertical lines indicate the entropy of the mean behavior for each trace;
prediction entropy must be compared to this.
In \Figure{packets-uncertainty}, the greater the reduction in uncertainty, the better.
}
\end{center}
\vspace{-2em}
\end{figure*}

As a better point-measure of the quality of a predictor, we propose the maximum vertical distance between its p-value \caps{CDF} and the ideal.
This is equivalent to choosing the worst possible predetermined significance level for each predictor and evaluating it at that level.
Under this measure, predictors cannot ``hide'' their errors by shifting them around the p-value range.
The maximum vertical difference from the ideal for each predictor is shown in \Figure{cdf} with vertical bars.
This is the the overall error rate of the predictor.
The success rate is the complement of the error rate.
\Figure{success-rates} shows the success rates of various predictions across the six data sets.
Our method predicts size and interval behaviors with accuracy between 70\% and 90\% across all data sets, while the other methods remain well below 50\% with few exceptions.

\begin{figure}[b]
\vspace{-1em}
\begin{center}
\includegraphics[width=3.5in]{pred_stats}
\caption{Success rates of various prediction methods.}
\fig{success-rates}
\end{center}
\vspace{-0.9em}
\end{figure}

Next we turn to the evaluation of packet counts predictions.
Here we cannot use the K-S test:
although we have a predicted distribution of possible packet counts, we have only one value to compare it to---the observed number of packets.
Instead we use the predicted distributions themselves to produce p-values for observed packet counts.
We compute how ``extreme'' each observed packet count value is in the predicted distribution of values:
what fraction of the distribution is as large or larger than the observed value.
If the predictions are accurate, then these ``p-values'' should be uniformly distributed.

\Figure{packets-accuracy} shows the \caps{CDF} of p-values of observed packet counts versus our predicted distributions.
While the \caps{CDF}s do not remain perfectly straight along the diagonal, the general tendency is for our predictions to remain fairly accurate.
There is some tendency for the observations to fall at the high and low ends of the predicted distributions, indicating that our predictions are somewhat too ``narrow.''
Nevertheless, the general nearness to the diagonal shows that, on the whole, our packet predictions remain fairly accurate.

Accuracy alone is insufficient, however:
guessing the mean distribution of packet counts yields a perfectly accurate predictor by this criterion.
What we want, however, is to remain accurate but become more precise in our predictions of packet count distributions.
We can measure this precision by computing the information-theoretic entropy of each distribution.
We want our accuracy to remain intact, but the entropy of our predictions to go down as much as possible.

\Figure{packets-entropy} shows the \caps{CDF}s of entropy values for our predictions across all six data sets.
The entropy of the average behavior of each set is indicated with a vertical line.
Entropy of predictions must be evaluated as compared with this.
In some cases, the entropy of our prediction is greater than the entropy of the average behavior.
In such cases, we can simply use the average behavior as a prediction instead.
Most of the time, however, we are gaining a great deal of information about what the actual packet count will be:
entropy is reduced to less than two bits or less about half of the time.

A closely related measure to entropy is the reduction in uncertainty.
The uncertainty of a prediction is two raised to the power of its entropy:
the number of possible choices the bits of entropy can encode.
The reduction in uncertainty is the fraction of the mean behavior's uncertainty that remains when we use our predictor.
This is the fraction of the original ``number'' of possible values that remain after our prediction as compared to before.
\Figure{packets-uncertainty} shows that across the six data sets, half of the time we discard between 75\% and 90\% of the original uncertainty.

\section{Discussion}\sec{discussion}

Our results show that we can predict the packet size distribution, inter-packet interval distribution, and packet count distribution of flows from only a handful of initial packets of each flow.
Specifically, we have used five initial packets to predict these distributions with between 70\% and 90\% accuracy across a wide variety of network traces.
Moreover, about half of the time, we can reduce our uncertainty about the number of packets that flows will have by 80\% or more.
Immediate applications of this ability to accurately predict individual flow behaviors abound.
Quality of service (\caps{QoS}) schemes attempting to provide high-quality service guarantees for streaming voice and video wireless through wireless networks could use such predictions to know, almost presciently, how much data flows are likely to transmit and how often.
Routers could use information about data rates from these predictions to load-balance across multiple possible routes towards their ultimate destinations.

While these potential applications are useful, the real significance of this work lies elsewhere.
Predicting flow behavior is merely a pleasant side effect of the real breakthrough:
a detailed statistical model for network-wide flow behaviors.
The fundamental concept underlying this work is that there are a relatively small collection of ``basic behaviors'' that generate the vast majority of the flow behaviors found in any particular trace.
These behaviors are associated with distributions of various flow features:
packet sizes, inter-packet intervals, \caps{IP} protocol, port numbers and packet counts.
These features are drawn from these distributions to create actual flows.
% We have treated the features as though they are sampled independently, but no doubt they are not really independent at all.

Given that the statistical model we have proposed for flow behavior is fairly simple---and finite mixture modeling is a well established field that has been studied since 1894~\cite{McLachlan00}.
Traditional approaches, however, have only applied to parametric probability distributions.
The distributions found in network traffic, however, are very distinctly non-parametric.
They are lumpy, unruly and generally quite ugly.
This is due largely to the fact that behaviors found in computer networks are not the by-products of smooth, natural phenomena, but rather of man-made artificial systems, following arbitrary, complex and capricious rules.

The numerical techniques needed to tackle non-parametric mixture modeling have only very recently become available.
In our model recovery procedure, we use a subspace segmentation algorithm published in 2007~\cite{Ma07}, two \caps{NMF} algorithms published in 2001 and 2008, respectively~\cite{Lee01,Kim08:anls}, and a low-rank \caps{NMF} initialization technique published for the first time here.
All of these techniques are still quite immature:
New \caps{NMF} algorithms have been regularly proposed since Lee and Seung's seminal paper in 2001.
Their algorithms are still the most well-known, and often the most reliable despite their various shortcomings~\cite{Langville06}.
they will only improve over time, yielding even better recovery of the model parameters.
Such improvements will immediately improve the performance of the prediction methods employed in this paper.

% Part of the reason the model of flow generation as mixtures of a basic set of behaviors is not obvious and has not previously been noted is likely that with convex linear combinations, so many different combinations can be created.
% Moreover, the analytical tools to extract this structure have only existed for a few years.
% In our model recovery procedure, we use a subspace segmentation algorithm published in 2007~\cite{Ma07}, two \caps{NMF} algorithms published in 2001 and 2008, respectively~\cite{Lee01,Kim08:anls}, and a low-rank \caps{NMF} initialization technique published for the first time here.
% And these techniques are still quite immature:
% they will only improve over time, yielding even better recovery of the model parameters.
% Such improvements will immediately improve the performance of the prediction methods employed in this paper.

It is our hope and belief that this model of flow behavior will provide future insights into other aspects of network traffic modeling:
in particular traffic classification and realistic workload generation.
The recovered model used in this paper is very close to providing a usable model for traffic generation already.
The largest missing piece is a model for realistic weight matrices.
If such applications based on this flow behavior model or other similar ones prove as fruitful as the application of flow behavior prediction, that will provide further evidence of the effectiveness and validity of our proposed model.

% TODO: mention the two dimension structure assumption.

\vfill
\pagebreak

\section{Related Work}\sec{related-work}

\section{Conclusions}\sec{conclusions}

\bibliography{IEEE,references}

\end{document}
